<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on SinLucidious</title>
    <link>/categories/programming/index.xml</link>
    <description>Recent content in Programming on SinLucidious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2017 - 2018, rlc4u; all rights reserved.</copyright>
    <atom:link href="/categories/programming/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Simply Said - Website Crawler</title>
      <link>/post/webcrawler/</link>
      <pubDate>Fri, 21 Apr 2017 13:50:46 +0200</pubDate>
      
      <guid>/post/webcrawler/</guid>
      <description>&lt;p&gt;Today, we are going to explore building a web scraper to mark a milestone in our progression towards learning GO. This exercise will be emphasizing on the skills that you should already be familiar with. It will cover skills such as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Scenario&lt;/em&gt; - Crawl a page and deeper into the relative urls and ensure that there are no dead links in any of your website.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sneak Preview&lt;/strong&gt;
&lt;img src=&#34;/images/snippet.png&#34; class=&#34;img-responsive center-block&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go run crawl_less.go http://example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Usage of reading a command line argument&lt;/li&gt;
&lt;li&gt;Scanning a webpage for links&lt;/li&gt;
&lt;li&gt;Going a level deeper and scanning for more links&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;//crawl.go
func main() {
	flag.Parse()

	args := flag.Args()
	fmt.Println(args)
	if len(args) &amp;lt; 1 {
		fmt.Println(&amp;quot;Please specify start page&amp;quot;)
		os.Exit(1)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the current main file, this will be the code that will be responsbile for taking in command arguments and checking to ensure that there is 1 url in the argument after running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go run crawl.go http://rlc4u.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, to spice things up further, we create a function called retrieve which will proceed to collect the full html elements of the indicated link and print it out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func retrieve(uri string) {

	resp, err := http.Get(uri)
	if err != nil {
		panic(err)
	}
	defer resp.Body.Close()

	body, _ := ioutil.ReadAll(resp.Body)
	fmt.Println(string(body))
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will now try to get the terminal to print that all links in the page which include relative links&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;links := collectlinks.All(resp.Body) // Here we use the collectlinks package
for _, link := range links {
  fmt.Println(link) // an iterator in many other languages get all links including relative
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These statements will proceed to run a number of iterations based on the numbers of links that the library has collected for us which includes relative link.&lt;/p&gt;

&lt;p&gt;Now that we have figured out what exactly, resp.body contains, we no longer need to see them anymore. The idea behind the next step would consist of extracting all elements containing the tag &amp;ldquo;a href&amp;rdquo;. This time, we will be using a library of &lt;a href=&#34;https://github.com/jackdanger/collectlinks&#34;&gt;Jack Danger&lt;/a&gt; that will do those extractions for us.&lt;/p&gt;

&lt;p&gt;Currently, we have a bunch of urls with no base url. We will now try to convert all relative urls to an absolute url.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func retrieve(uri string) {
...
    ...
	for _, link := range links {
		absolute := fixUrl(link, uri)
		fmt.Println(absolute)
	}
}

func fixUrl(href, base string) string {
	uri, err := url.Parse(href)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	baseUrl, err := url.Parse(base)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	uri = baseUrl.ResolveReference(uri)
	return uri.String()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, once we print our urls, we will see thats all our paths that was relative is now converted to absolute urls&lt;/p&gt;

&lt;p&gt;Going deeper, we want to ensure that the links to want to crawl deeper are accessible and we proceed to add in the following code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// retrieve.go
...
  ...
for _, link := range links {
  //fmt.Println(link) // an iterator in many other languages get all links including relative
  absolute := fixUrl(link, uri)
  //fmt.Println(absolute)
  resp, _ := http.Get(absolute)

  switch resp.StatusCode {
  case 200:
    fmt.Println(&amp;quot;[Up] \t\t&amp;quot;, absolute)
  default:
    fmt.Println(&amp;quot;[Down] \t\t&amp;quot;, absolute)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We make use of maps in order to prevent the crawler from digging deeper into the same url which would prevent an endless loop. A Sample of what it looks like is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;visitedPage   = make(map[string]bool)

if strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedPage[absolute] {
			checkWebStatus(absolute, uri)
		}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We proceed to upgrade our retrieve.go in such a way that we throw the url into a queue and allows our crawler to go digging into the urls that it has found The method we are using is recursive.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//replacement of retrieve.go
func enqueue(uri string) {
	//fmt.Println(&amp;quot;fetching&amp;quot;, uri)
	visitedPage[uri] = true
	resp, err := http.Get(uri)
	if err != nil {
		return
	}
	defer resp.Body.Close()
	links := collectlinks.All(resp.Body)
	for _, link := range links {
		absolute := fixUrl(link, uri)
		if !strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedLinks[absolute] {
			visitedLinks[absolute] = true
			checkWebStatus(absolute, uri)
		}
		if strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedPage[absolute] {
			UrlCrawlCount++
			//fmt.Println(absolute)
			checkWebStatus(absolute, uri)
			enqueue(absolute)
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you read the code, you will realize that we have applied the same theory to links so as to prevent checking the web status of the same link twice. With that, you have completed a fully-functional web crawler that is capable of digging into your website ensuring that the links in them are kept up to date.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

//Recurisve Web Crawling
import (
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/briandowns/spinner&amp;quot;
	&amp;quot;github.com/jackdanger/collectlinks&amp;quot;
	&amp;quot;net/http&amp;quot;
	&amp;quot;net/url&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;strings&amp;quot;
	&amp;quot;time&amp;quot;
)

var (
	visitedPage   = make(map[string]bool)
	visitedLinks  = make(map[string]bool)
	BrokenPage    = make(map[string]string)
	UrlCrawlCount = 0
	Linkcount     = 0
	brokenLinks   = 0
	baseURL       = &amp;quot;&amp;quot;
)

func main() {
	flag.Parse()
	args := flag.Args()
	//fmt.Println(args)
	if len(args) &amp;lt; 1 {
		fmt.Println(&amp;quot;Please specify start page&amp;quot;)
		os.Exit(1)
	}
	baseURL = args[0]

	s := spinner.New(spinner.CharSets[9], 100*time.Millisecond)
	s.Prefix = fmt.Sprintf(&amp;quot;crawling %s, please wait &amp;quot;, baseURL)
	s.Start()
	enqueue(baseURL)

	tmpCount := 0
	s.Stop()
	fmt.Println(&amp;quot;================================================================================================&amp;quot;)
	fmt.Println(&amp;quot;================================================================================================&amp;quot;)
	fmt.Println(&amp;quot;Broken Links:&amp;quot;, brokenLinks, &amp;quot;Ok Links:&amp;quot;, Linkcount, &amp;quot;Web Pages Crawled:&amp;quot;, UrlCrawlCount)
	for key, value := range BrokenPage {
		tmpCount++
		fmt.Println(fmt.Sprintf(&amp;quot;[%v] \n broken  : %s \n source: %s&amp;quot;, tmpCount, key, value))
	}
}

// fixUrl converts all relative links to absolute links
func fixUrl(href, base string) string {
	uri, err := url.Parse(href)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	baseUrl, err := url.Parse(base)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	uri = baseUrl.ResolveReference(uri)
	return uri.String()
}

func enqueue(uri string) {
	//fmt.Println(&amp;quot;fetching&amp;quot;, uri)
	visitedPage[uri] = true
	resp, err := http.Get(uri)
	if err != nil {
		return
	}
	defer resp.Body.Close()
	links := collectlinks.All(resp.Body)
	for _, link := range links {
		absolute := fixUrl(link, uri)
		if !strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedLinks[absolute] {
			visitedLinks[absolute] = true
			checkWebStatus(absolute, uri)
		}
		if strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedPage[absolute] {
			UrlCrawlCount++
			//fmt.Println(absolute)
			checkWebStatus(absolute, uri)
			enqueue(absolute)
		}
	}
}

// checkWebStatus checks all given links if they are invalid
func checkWebStatus(urlParams string, baseline string) {
	resp, _ := http.Get(urlParams)
	if resp != nil &amp;amp;&amp;amp; resp.StatusCode == 200 {
		Linkcount++
	} else {
		brokenLinks++
		BrokenPage[urlParams] = baseline
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get the full source code from &lt;a href=&#34;https://github.com/richmondgoh8/web-crawler&#34;&gt;My Github Repo&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simply Said - China Infrastructure Deployment</title>
      <link>/post/china-deployment/</link>
      <pubDate>Fri, 14 Apr 2017 13:50:46 +0200</pubDate>
      
      <guid>/post/china-deployment/</guid>
      <description>&lt;p&gt;In this blog, we will be exploring the way of deploying the current service infrasture that you have built locally to deploying it on a server in china via Alibaba Cloud aka Aliyun.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Before starting on this tutorial, the following should be done:&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Services Created and running locally.&lt;/li&gt;
&lt;li&gt;Ensure DockerFile have been build.&lt;/li&gt;
&lt;li&gt;Understanding of &lt;a href=&#34;https://docs.docker.com/engine/getstarted/step_four/&#34;&gt;Docker&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://hub.docker.com/_/haproxy/&#34;&gt;HAProxy&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Successful Implementation&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Register an account with both Aliyun CN and INTL Account&lt;/li&gt;
&lt;li&gt;Login into the CN account &amp;amp; Create an &lt;a href=&#34;https://m.aliyun.com/yunqi/articles/9102&#34;&gt;Image WareHouse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;At this point, you should have successful push the built docker image to aliyun registry store.&lt;/li&gt;
&lt;li&gt;Now, we proceed to Creating an ECS Instance (2 core, 8GB Ram, Ubuntu 16.0.4)&lt;/li&gt;
&lt;li&gt;After creating an instance, you will be given an Internet Address of that Instance.&lt;/li&gt;
&lt;li&gt;Proceed to SSH (or Putty for Windows) into the virtual machine.&lt;/li&gt;
&lt;li&gt;Install &lt;a href=&#34;https://docs.docker.com/engine/getstarted/step_one/&#34;&gt;Docker&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;Docker Compose&lt;/a&gt; in the ECS Machine&lt;/li&gt;
&lt;li&gt;Create a &lt;a href=&#34;https://docs.docker.com/compose/compose-file/compose-file-v2/#dependson&#34;&gt;docker-compose.yml&lt;/a&gt; file in a directory in the Ubuntu Machine&lt;/li&gt;
&lt;li&gt;Ensure that HAProxy is the first image you build. You can check the statistics after building the image via http://”Internet IP”/stats/haproxy&lt;/li&gt;
&lt;li&gt;Run&lt;code&gt;Docker-Compose Up&lt;/code&gt; and ensure that services are connected and working.&lt;/li&gt;
&lt;li&gt;Enjoy&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;FAQ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why do we have both an intl and cn aliyun account?&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Currently the intl account do not have the the registry image warehouse and is only available in the cn aliyun account. The reason why we do not do everything in the cn aliyun account is because it requires a &amp;ldquo;real-name&amp;rdquo; authentication that we currently do not possess and is unable to do.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Why do we not use container service?&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Facing difficulties with Container Services linking multi containers services. Creating a custom image from the services provided in the international account could not pull from the registry properly. Orchestration templates formats in building the multi containers link file have to be in Aliyun’s format context.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Why did we have to set up OpenVPN through Digital Ocean?&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Accessing Aliyun Cloud Website from Singapore would redirect us to the Internal Version of the website which did not provide us with the features we require. Proxy would not work as it does not enable scripting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Why didn’t we use third party registry such as docker hub?&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The main reason behind not using third-party registry is that they are costly. Another reason is that we will never know when the china’s firewall will block connection from external sources. Integration with third party registry might not be as optimised. Moreover, ensuring all registries have to be in ‘private’ mode.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Simply Said - Deploying Hugo On AWS</title>
      <link>/post/deploy-blog/</link>
      <pubDate>Fri, 31 Mar 2017 13:50:46 +0200</pubDate>
      
      <guid>/post/deploy-blog/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Initially, I have been using Web Hosting Hub with wordpress installed with it to run my website. I got my domain name from OpenSRS. I am quite happy with their services and support. However, It has come to the point that I am clearly paying too much for just hosting a blog which stood at about 13 bucks a month.&lt;/p&gt;

&lt;p&gt;
&lt;p&gt;This is where I chanced upon Amazon Web Services. Not being a good front-end developer, I went around in search of a template where I found Hugo, a static web builder developed with Golang. Below list the ways i went around trying to switch from web hosting hub to AWS. It was a rather fruitful experience as my overhead became less than 1 dollar a month.&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Feel free to view a step by step tutorial on deploying hugo with Amazon Web Services by &lt;a href=&#34;http://bezdelev.com/post/hugo-aws-lambda-static-website/&#34;&gt;Ilya Bezdelev&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In a nutshell, key points that you should note involves:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hugo website

&lt;ul&gt;
&lt;li&gt;Follow Hugo Tutorial on Creating Website &lt;a href=&#34;https://gohugo.io/overview/quickstart/&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;example.com bucket

&lt;ul&gt;
&lt;li&gt;serves the website content&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;www.example.com bucket

&lt;ul&gt;
&lt;li&gt;redirects to example.com endpoint&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;input.example.com bucket&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;contains all hugo related files&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Default AWS Endpoint Domain Name&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Storage Buckets (used for containing Hugo files and serving web pages to visitors to site)&lt;/li&gt;
&lt;li&gt;Lambda ( piece of code that triggers downloading of static files from hugo input to output bucket)
&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Custom Domain Name&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;CloudFront Distribution (CDN)&lt;/li&gt;
&lt;li&gt;Route 53
&lt;/br&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Security of AWS resources will be managed by Identity &amp;amp; Access Management(IAM)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;AWS CloudFront consist of CDN which helps to speed up your website by&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Route 53 is used as a DNS Service as well as Domain Name Registration&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Amazon Route 53 is an authoritative Domain Name System (DNS) service. DNS is the system that translates human-readable domain names (example.com) into IP addresses (192.0.2.0). With authoritative name servers in data centers all over the world, Route 53 is reliable, scalable, and fast.
TLD = Top Level Domain&lt;/p&gt;

&lt;p&gt;With all these in hand, Begin your journey to hosting your very own blog.&lt;/p&gt;

&lt;p&gt;Remember &amp;ndash;&amp;gt; Create Hugo Website &amp;ndash;&amp;gt; Create AWS Web Service &amp;ndash;&amp;gt; Create S3 to store Hugo Files &amp;ndash;&amp;gt; DNS with hosted zone to serve your example.com to the world.&lt;br /&gt;
&lt;/br&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Disclaimer: feel free to read up more about &lt;a href=&#34;https://gohugo.io/overview/introduction/&#34;&gt;Hugo&lt;/a&gt; to find out about the other ways to host Hugo files. A free alternative would be netlify!&lt;/em&gt;
&lt;em&gt;PS - Web Hosting Hub Support is Excellent&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Simply Said - Types of Developers</title>
      <link>/post/developer-stack/</link>
      <pubDate>Fri, 17 Mar 2017 13:50:46 +0200</pubDate>
      
      <guid>/post/developer-stack/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Congratulations to taking your first step into the world of development. When you hear the word &amp;ldquo;programmer&amp;rdquo;, what first comes to your mind? Are there different type of programmers? If they are, what kind of programmers are they?&lt;/p&gt;

&lt;p&gt;
Generically speaking, there are many different types of programmers. Each with different roles, skillsets and mindset. In today&amp;rsquo;s context, I will be talking about 5 different kind of programmers. So lets begin!&lt;/p&gt;

&lt;p&gt;Programmers are broken down into:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Web Developers

&lt;ul&gt;
&lt;li&gt;Short Answer: They build and maintain website. You can get a more detailed explaination &lt;a href=&#34;http://www.theodinproject.com/courses/introduction-to-web-development/lessons/what-a-web-developer-does&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Application Developer&lt;/li&gt;
&lt;li&gt;Game Developer&lt;/li&gt;
&lt;li&gt;UI Developer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When you branch into any of the fields, all of them will have 2 different sub-categories which will be front-end and back-end developer. So your title could be something like a Front-End Web Developer or a Back-End Application Developer. For the more ambitious, they can proceed to becoming a full-stack developer whereby they would be able to handle both the front-end and back-end aspect of the application that they are building.&lt;/p&gt;

&lt;h3 id=&#34;important-distinctions&#34;&gt;Important Distinctions&lt;/h3&gt;

&lt;h4 id=&#34;front-end-developer&#34;&gt;Front-End Developer?&lt;/h4&gt;

&lt;p&gt;&amp;ldquo;Front End&amp;rdquo; typically refers to the stuff that you actually see on the website in the browser. Languages that they primarily focus on includes: HTML, CSS, Javascript, JQuery and React.js. Despite being in charge of making things that appear to the visitor&amp;rsquo;s eyes, the &amp;ldquo;pretty&amp;rdquo; website is still designed by another person with the role of a &amp;ldquo;designer&amp;rdquo;. Some of the things they do include making buttons, menus, formatting of texts, transitions and modals.&lt;/p&gt;

&lt;h4 id=&#34;back-end-developer&#34;&gt;Back-End Developer?&lt;/h4&gt;

&lt;p&gt;&amp;ldquo;Back End&amp;rdquo; typically refers to the stuff that happens behind-the-scene. They are more focused on connecting the website request to the correct services which would include ensuring that the right data are passing through the browswer, connecting services to databases, implementing infrasture and mapping the ports of the services. They ensure that all the buttons and data collected are moving as intended.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>