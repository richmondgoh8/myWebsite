<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on SinLucidious</title>
    <link>/tags/projects/index.xml</link>
    <description>Recent content in Projects on SinLucidious</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Richmond Goh</copyright>
    <atom:link href="/tags/projects/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Building a Website Crawler</title>
      <link>/project/WebCrawler/</link>
      <pubDate>Fri, 21 Apr 2017 13:50:46 +0200</pubDate>
      
      <guid>/project/WebCrawler/</guid>
      <description>&lt;p&gt;Today, we are going to explore building a web scraper to mark a milestone in our progression towards learning GO. This exercise will be emphasizing on the skills that you should already be familiar with. It will cover skills such as:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Scenario&lt;/em&gt; - Crawl a page and deeper into the relative urls and ensure that there are no dead links in any of your website.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sneak Preview&lt;/strong&gt;
&lt;img src=&#34;/images/snippet.png&#34; class=&#34;img-responsive center-block&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go run crawl_less.go http://example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Usage of reading a command line argument&lt;/li&gt;
&lt;li&gt;Scanning a webpage for links&lt;/li&gt;
&lt;li&gt;Going a level deeper and scanning for more links&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;//crawl.go
func main() {
	flag.Parse()

	args := flag.Args()
	fmt.Println(args)
	if len(args) &amp;lt; 1 {
		fmt.Println(&amp;quot;Please specify start page&amp;quot;)
		os.Exit(1)
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the current main file, this will be the code that will be responsible for taking in command arguments and checking to ensure that there is 1 url in the argument after running&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;go run crawl.go http://rlc4u.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, to spice things up further, we create a function called retrieve which will proceed to collect the full html elements of the indicated link and print it out.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func retrieve(uri string) {

	resp, err := http.Get(uri)
	if err != nil {
		panic(err)
	}
	defer resp.Body.Close()

	body, _ := ioutil.ReadAll(resp.Body)
	fmt.Println(string(body))
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will now try to get the terminal to print that all links in the page which include relative links&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;links := collectlinks.All(resp.Body) // Here we use the collectlinks package
for _, link := range links {
  fmt.Println(link) // an iterator in many other languages get all links including relative
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These statements will proceed to run a number of iterations based on the numbers of links that the library has collected for us which includes relative link.&lt;/p&gt;

&lt;p&gt;Now that we have figured out what exactly, resp.body contains, we no longer need to see them anymore. The idea behind the next step would consist of extracting all elements containing the tag &amp;ldquo;a href&amp;rdquo;. This time, we will be using a library of &lt;a href=&#34;https://github.com/jackdanger/collectlinks&#34;&gt;Jack Danger&lt;/a&gt; that will do those extractions for us.&lt;/p&gt;

&lt;p&gt;Currently, we have a bunch of urls with no base url. We will now try to convert all relative urls to an absolute url.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func retrieve(uri string) {
...
    ...
	for _, link := range links {
		absolute := fixUrl(link, uri)
		fmt.Println(absolute)
	}
}

func fixUrl(href, base string) string {
	uri, err := url.Parse(href)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	baseUrl, err := url.Parse(base)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	uri = baseUrl.ResolveReference(uri)
	return uri.String()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, once we print our urls, we will see thats all our paths that was relative is now converted to absolute urls&lt;/p&gt;

&lt;p&gt;Going deeper, we want to ensure that the links to want to crawl deeper are accessible and we proceed to add in the following code.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// retrieve.go
...
  ...
for _, link := range links {
  //fmt.Println(link) // an iterator in many other languages get all links including relative
  absolute := fixUrl(link, uri)
  //fmt.Println(absolute)
  resp, _ := http.Get(absolute)

  switch resp.StatusCode {
  case 200:
    fmt.Println(&amp;quot;[Up] \t\t&amp;quot;, absolute)
  default:
    fmt.Println(&amp;quot;[Down] \t\t&amp;quot;, absolute)
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We make use of maps in order to prevent the crawler from digging deeper into the same url which would prevent an endless loop. A Sample of what it looks like is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;visitedPage   = make(map[string]bool)

if strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedPage[absolute] {
			checkWebStatus(absolute, uri)
		}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We proceed to upgrade our retrieve.go in such a way that we throw the url into a queue and allows our crawler to go digging into the urls that it has found The method we are using is recursive.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//replacement of retrieve.go
func enqueue(uri string) {
	//fmt.Println(&amp;quot;fetching&amp;quot;, uri)
	visitedPage[uri] = true
	resp, err := http.Get(uri)
	if err != nil {
		return
	}
	defer resp.Body.Close()
	links := collectlinks.All(resp.Body)
	for _, link := range links {
		absolute := fixUrl(link, uri)
		if !strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedLinks[absolute] {
			visitedLinks[absolute] = true
			checkWebStatus(absolute, uri)
		}
		if strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedPage[absolute] {
			UrlCrawlCount++
			//fmt.Println(absolute)
			checkWebStatus(absolute, uri)
			enqueue(absolute)
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you read the code, you will realize that we have applied the same theory to links so as to prevent checking the web status of the same link twice. With that, you have completed a fully-functional web crawler that is capable of digging into your website ensuring that the links in them are kept up to date.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;package main

//Recurisve Web Crawling
import (
	&amp;quot;flag&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/briandowns/spinner&amp;quot;
	&amp;quot;github.com/jackdanger/collectlinks&amp;quot;
	&amp;quot;net/http&amp;quot;
	&amp;quot;net/url&amp;quot;
	&amp;quot;os&amp;quot;
	&amp;quot;strings&amp;quot;
	&amp;quot;time&amp;quot;
)

var (
	visitedPage   = make(map[string]bool)
	visitedLinks  = make(map[string]bool)
	BrokenPage    = make(map[string]string)
	UrlCrawlCount = 0
	Linkcount     = 0
	brokenLinks   = 0
	baseURL       = &amp;quot;&amp;quot;
)

func main() {
	flag.Parse()
	args := flag.Args()
	//fmt.Println(args)
	if len(args) &amp;lt; 1 {
		fmt.Println(&amp;quot;Please specify start page&amp;quot;)
		os.Exit(1)
	}
	baseURL = args[0]

	s := spinner.New(spinner.CharSets[9], 100*time.Millisecond)
	s.Prefix = fmt.Sprintf(&amp;quot;crawling %s, please wait &amp;quot;, baseURL)
	s.Start()
	enqueue(baseURL)

	tmpCount := 0
	s.Stop()
	fmt.Println(&amp;quot;================================================================================================&amp;quot;)
	fmt.Println(&amp;quot;================================================================================================&amp;quot;)
	fmt.Println(&amp;quot;Broken Links:&amp;quot;, brokenLinks, &amp;quot;Ok Links:&amp;quot;, Linkcount, &amp;quot;Web Pages Crawled:&amp;quot;, UrlCrawlCount)
	for key, value := range BrokenPage {
		tmpCount++
		fmt.Println(fmt.Sprintf(&amp;quot;[%v] \n broken  : %s \n source: %s&amp;quot;, tmpCount, key, value))
	}
}

// fixUrl converts all relative links to absolute links
func fixUrl(href, base string) string {
	uri, err := url.Parse(href)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	baseUrl, err := url.Parse(base)
	if err != nil {
		return &amp;quot;&amp;quot;
	}
	uri = baseUrl.ResolveReference(uri)
	return uri.String()
}

func enqueue(uri string) {
	//fmt.Println(&amp;quot;fetching&amp;quot;, uri)
	visitedPage[uri] = true
	resp, err := http.Get(uri)
	if err != nil {
		return
	}
	defer resp.Body.Close()
	links := collectlinks.All(resp.Body)
	for _, link := range links {
		absolute := fixUrl(link, uri)
		if !strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedLinks[absolute] {
			visitedLinks[absolute] = true
			checkWebStatus(absolute, uri)
		}
		if strings.Contains(absolute, baseURL) &amp;amp;&amp;amp; !visitedPage[absolute] {
			UrlCrawlCount++
			//fmt.Println(absolute)
			checkWebStatus(absolute, uri)
			enqueue(absolute)
		}
	}
}

// checkWebStatus checks all given links if they are invalid
func checkWebStatus(urlParams string, baseline string) {
	resp, _ := http.Get(urlParams)
	if resp != nil &amp;amp;&amp;amp; resp.StatusCode == 200 {
		Linkcount++
	} else {
		brokenLinks++
		BrokenPage[urlParams] = baseline
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Get the full source code from &lt;a href=&#34;https://github.com/richmondgoh8/web-crawler&#34;&gt;My Github Repo&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Daily Dose</title>
      <link>/project/DailyDose/</link>
      <pubDate>Tue, 18 Nov 2014 02:13:50 +0000</pubDate>
      
      <guid>/project/DailyDose/</guid>
      <description>

&lt;h2 id=&#34;synopsis&#34;&gt;Synopsis&lt;/h2&gt;

&lt;p&gt;This app helps you to keep track of all your daily habits so you would never forget them. You will also be able to ensure if you have reached your optimal nutrition and vitamin which will help you maximize your weight loss goals.&lt;/p&gt;

&lt;p&gt;★ No Ads
★ Ensure you have completed your daily regime to stay productive
★ Add, Remove Daily Tasks
★ Ability to check progression of daily tasks and nutrition requirement
★ Free on all devices!&lt;/p&gt;

&lt;p&gt;Daily Dose is currently in beta phase, stay tuned for more updates and features!&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://play.google.com/store/apps/details?id=com.gmail.richmondgoh.dailyDose&#34;&gt;https://play.google.com/store/apps/details?id=com.gmail.richmondgoh.dailyDose&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fit4life</title>
      <link>/project/Fit4Life/</link>
      <pubDate>Tue, 18 Nov 2014 02:13:50 +0000</pubDate>
      
      <guid>/project/Fit4Life/</guid>
      <description>

&lt;h2 id=&#34;synopsis&#34;&gt;Synopsis&lt;/h2&gt;

&lt;p&gt;Are you currently on the hunt for workout routines that could make a difference in your life? Look no further as Fit4Life provides you with anything you need to maximize your gains and become fit in the shortest time possible.&lt;/p&gt;

&lt;p&gt;★ Optimized workouts that helps to maximize your gains
★ Short &amp;amp; Sweet workout routines that ends in less than 1 hour
★ Fit4Life is free on all devices!&lt;/p&gt;

&lt;p&gt;Fit4Life is currently in beta phase, stay tuned for pushup counters, more routines and many more! Fit4Life cannot be done without the help of darabee and rlc4u. Do check out their websites for more information!&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://play.google.com/store/apps/details?id=com.gmail.richmondgoh.Fit4Life&#34;&gt;https://play.google.com/store/apps/details?id=com.gmail.richmondgoh.Fit4Life&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sleep Tight</title>
      <link>/project/sleepTight/</link>
      <pubDate>Tue, 18 Nov 2014 02:13:50 +0000</pubDate>
      
      <guid>/project/sleepTight/</guid>
      <description>

&lt;h2 id=&#34;synopsis&#34;&gt;Synopsis&lt;/h2&gt;

&lt;p&gt;This app helps to calculate the best time for you to sleep and wake up based on your needs. Being lethargic in the morning will be a thing in the past! The more sleep cycle you sleep through, the more refreshed you will feel.&lt;/p&gt;

&lt;p&gt;Sleep better and wake up ready to seize the day! Stay tuned for more upcoming features such as tracking your sleep, alarms and many more!&lt;/p&gt;

&lt;p&gt;★ Helps you find the optimal time to wake up!
★ Helps you find the optimal time to wake sleep!
★ Know when to sleep or wake up no matter how much time you have!&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://play.google.com/store/apps/details?id=com.gmail.richmondgoh.sleepTight&#34;&gt;https://play.google.com/store/apps/details?id=com.gmail.richmondgoh.sleepTight&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>